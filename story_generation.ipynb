{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYAHRLWNXAx7"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# -outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "#outputs, states = tf.compat.v1.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USTzO6bDXOiP"
   },
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\srika\\Dropbox\\PC\\Downloads\\Story-Generation-master (1)\\Story-Generation-master\\NLP CORPUS UNEDITED.txt\") as f:\n",
    "  text=f.read()\n",
    "  \n",
    "vocab=sorted(set(text))\n",
    "vocab_2_int={ch:i for i,ch in enumerate(vocab)}\n",
    "int_2_vocab=dict(enumerate(vocab))\n",
    "with open(r\"C:\\Users\\srika\\Dropbox\\PC\\Downloads\\Story-Generation-master (1)\\Story-Generation-master\\NLP CORPUS UNEDITED.txt\") as f:\n",
    "  text1=f.read()\n",
    "encoded=np.array([vocab_2_int[ch] for ch in text1],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2yOIaszYTT0",
    "outputId": "7a81939c-042a-4cb2-fafe-74ff71bae475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n                        THE ADVENTURE OF THE ABBEY GRANGE\\n\\n                               Arthur'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "xZeQd-TOY5YR",
    "outputId": "9e3c2631-155a-4d46-c367-72b080637fec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 33, 30,  1, 26, 29,\n",
       "       47, 30, 39, 45, 46, 43, 30,  1, 40, 31,  1, 45, 33, 30,  1, 26, 27,\n",
       "       27, 30, 50,  1, 32, 43, 26, 39, 32, 30,  0,  0,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, 26, 72, 74, 62, 75, 72])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TbMJ_Av_ZG7J",
    "outputId": "db199d6e-99b7-4dd7-c539-d5d33b8b2f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3pcmgYRa972"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr,batch_size,n_steps):\n",
    "  \n",
    "  chars_per_batch=batch_size*n_steps\n",
    "  n_batches=len(arr)//chars_per_batch\n",
    "  \n",
    "  arr=arr[:n_batches*chars_per_batch]\n",
    "  # Reshape into batch_size rows\n",
    "  arr=arr.reshape((batch_size,-1))\n",
    "  \n",
    "  for n in range(0,arr.shape[1],n_steps):\n",
    "    x=arr[:,n:n+n_steps]\n",
    "    \n",
    "    y_temp=arr[:,n+1:n+n_steps+1]\n",
    "    \n",
    "    y=np.zeros(x.shape,dtype=x.dtype)\n",
    "    \n",
    "    y[:,:y_temp.shape[1]]=y_temp\n",
    "    \n",
    "    yield x,y\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjEReS-edFSb"
   },
   "outputs": [],
   "source": [
    "batches=get_batches(encoded,10,50)\n",
    "x,y=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "lkyUer5GdT2u",
    "outputId": "c276603f-4527-47c4-baf7-affd782575ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " [[ 0  0  0  0  1  1  1  1  1  1]\n",
      " [58  1 60 69 72  1 74 62 63 73]\n",
      " [68 74 55 57 74  1 77 63 74 62]\n",
      " [60  1 63 74  1 63 73  1 68 69]\n",
      " [ 1 55 73 73 55 73 73 63 68  1]\n",
      " [59 58 11  1  3 29 63 58  1 74]\n",
      " [69 65 59 68  1 68 69 74  1 60]\n",
      " [63 68 61  1 58 75 67 56 10 56]\n",
      " [ 1 63 68  1 61 69 66 58  9  1]\n",
      " [55 58  1 44 63 72  1 35 69 62]]\n",
      "\n",
      "y=\n",
      " [[ 0  0  0  1  1  1  1  1  1  1]\n",
      " [ 1 60 69 72  1 74 62 63 73  1]\n",
      " [74 55 57 74  1 77 63 74 62  1]\n",
      " [ 1 63 74  1 63 73  1 68 69 74]\n",
      " [55 73 73 55 73 73 63 68  1 57]\n",
      " [58 11  1  3 29 63 58  1 74 62]\n",
      " [65 59 68  1 68 69 74  1 60 55]\n",
      " [68 61  1 58 75 67 56 10 56 59]\n",
      " [63 68  1 61 69 66 58  9  1 55]\n",
      " [58  1 44 63 72  1 35 69 62 68]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x=\\n\",x[:10,:10])\n",
    "print(\"\\ny=\\n\",y[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7pBlLFBdo8R"
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size,num_steps):\n",
    "  inputs=tf.placeholder(tf.int32,[batch_size,num_steps],name=\"inputs\")\n",
    "  target=tf.placeholder(tf.int32,[batch_size,num_steps],name=\"targets\")\n",
    "  \n",
    "  keep_prob=tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "  \n",
    "  return inputs,target,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ8JQ_UserHU"
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "  \n",
    "  def build_cell(lstm_size,keep_prob):\n",
    "    lstm=tf.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop=tf.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "    return drop\n",
    "  #stack multiple lstm layers\n",
    "  cell=tf.rnn.MultiRNNCell([build_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
    "  \n",
    "  initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "  \n",
    "  return cell,initial_state\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Wo9qFSlgWiV"
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output,in_size,out_size):\n",
    "#   in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "#   out_size: Size of this softmax layer\n",
    "  \n",
    "  seq_output=tf.concat(lstm_output,axis=1)\n",
    "#   x :input tensor\n",
    "  x=tf.reshape(seq_output,[-1,in_size])\n",
    "  \n",
    "  with tf.variable_scope('softmax'):\n",
    "    weight=tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
    "    bias=tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    \n",
    "  logits=tf.matmul(x,weight)+bias\n",
    "  \n",
    "  out=tf.softmax(logits,name=\"prediction\")\n",
    "  \n",
    "  return out,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeKXk7mmiB4w"
   },
   "outputs": [],
   "source": [
    "def build_loss(logits,targets,lstm_size,num_characters):\n",
    "  \n",
    "  y_one_hot=tf.one_hot(targets,num_characters)\n",
    "  y_reshaped=tf.reshape(y_one_hot,logits.get_shape())\n",
    "  \n",
    "  loss=tf.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped)\n",
    "  loss=tf.reduce_mean(loss)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeteTOflitWo"
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss,learning_rate,grad_clip):\n",
    "  tvars=tf.trainable_variables()\n",
    "  grads,_=tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)\n",
    "  train_optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "  optimizer=train_optimizer.apply_gradients(zip(grads,tvars))\n",
    "  \n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjVJ0SACjwbV"
   },
   "outputs": [],
   "source": [
    "class charRNN:\n",
    "  def __init__(self,num_characters,batch_size=64,num_steps=50,lstm_size=128,num_layers=2,learning_rate=0.01,grad_clip=5,sampling=False):\n",
    "    \n",
    "    if sampling==True:\n",
    "      batch_size,num_steps=1,1\n",
    "      \n",
    "    else:\n",
    "      batch_size,num_steps=batch_size,num_steps\n",
    "      \n",
    "    #tf.reset_default_graph()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    self.inputs,self.targets,self.keep_prob=build_inputs(batch_size,num_steps)\n",
    "    \n",
    "    cell,self.initial_state=build_lstm(lstm_size,num_layers,batch_size,self.keep_prob)\n",
    "    \n",
    "    #     Run the data through the RNN layers\n",
    "    #     First, one-hot encode the input tokens\n",
    "    \n",
    "    x_one_hot=tf.one_hot(self.inputs,num_characters)\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    \n",
    "    outputs,state=tf.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)  \n",
    "    self.final_state=state\n",
    "    \n",
    "    self.prediction,self.logits=build_output(outputs,lstm_size,num_characters)\n",
    "    \n",
    "    self.loss=build_loss(self.logits,self.targets,lstm_size,num_characters)\n",
    "    \n",
    "    self.optimizer=build_optimizer(self.loss,learning_rate,grad_clip)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ypn_C9UVneW7"
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "num_steps=100\n",
    "num_layers=2\n",
    "lstm_size=512\n",
    "keep_prob=0.5\n",
    "learning_rate=0.001\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "colab_type": "code",
    "id": "bCY50TSSntcc",
    "outputId": "8c090e50-14d0-4b00-f2b8-de9caacfb262"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13380\\3583610886.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = charRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[0mlstm_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 learning_rate=learning_rate)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13380\\4037869571.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_characters, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, sampling)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#     Run the data through the RNN layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13380\\544703431.py\u001b[0m in \u001b[0;36mbuild_lstm\u001b[1;34m(lstm_size, num_layers, batch_size, keep_prob)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;31m#stack multiple lstm layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m   \u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuild_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'rnn'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = charRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver=tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  count=0\n",
    "  for e in range(epochs):\n",
    "    new_state=sess.run(model.initial_state)\n",
    "    loss=0\n",
    "    \n",
    "    for x,y in get_batches(encoded,batch_size,num_steps):\n",
    "      count=count+1\n",
    "      feed_dict={model.inputs:x,model.targets:y,model.keep_prob:keep_prob,model.initial_state:new_state}\n",
    "      \n",
    "      batch_loss,new_state,_ = sess.run([model.loss,model.final_state,model.optimizer],feed_dict=feed_dict)\n",
    "      \n",
    "      if(count%50==0):\n",
    "        print(\"Epoch:{0} , loss:{1}\".format(e+1,batch_loss))\n",
    "        \n",
    "      if(count%200==0):\n",
    "        saver.save(sess,\"checkpoints/i{}_l{}.ckpt\".format(count,lstm_size))\n",
    "        \n",
    "  saver.save(sess,\"checkpoints1/i{}_l{}.ckpt\".format(count,lstm_size))\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "JkcWiCccrqmA",
    "outputId": "d06ce8be-b91f-4a84-86bc-aeef1ee5b56e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints1/i1980_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/../checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints1/i1980_l512.ckpt\""
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qn_4aGbuBWz"
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds,vocab_size,top_n=5):\n",
    "  p=np.squeeze(preds)\n",
    "  p[np.argsort(p)[:-top_n]]=0\n",
    "  p=p/np.sum(p)\n",
    "  c=np.random.choice(vocab_size,1,p=p)[0]\n",
    "  \n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woJx6nQ0vHGf"
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint,n_samples,lstm_size,vocab_size,prime='the '):\n",
    "  \n",
    "  samples=[ch for ch in prime]\n",
    "  model=charRNN(len(vocab),lstm_size=lstm_size,sampling=True)\n",
    "  saver=tf.train.Saver()\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    saver.restore(sess,checkpoint)\n",
    "    new_state=sess.run(model.initial_state)\n",
    "    \n",
    "    for ch in prime:\n",
    "      x=np.zeros((1,1))\n",
    "      x[0,0]=vocab_2_int[ch]\n",
    "      feed_dict={model.inputs:x , model.keep_prob:1.0 ,model.initial_state: new_state}\n",
    "      preds,new_state=sess.run([model.prediction,model.final_state],feed_dict=feed_dict)\n",
    "\n",
    "    ch=pick_top_n(preds,len(vocab))\n",
    "    samples.append(int_2_vocab[ch])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "      x[0,0]=ch\n",
    "          \n",
    "      feed_dict={model.inputs:x , model.keep_prob:1.0 ,model.initial_state:new_state}\n",
    "    \n",
    "      preds,new_state=sess.run([model.prediction,model.final_state],feed_dict=feed_dict)\n",
    "      \n",
    "      ch=pick_top_n(preds,len(vocab))\n",
    "      samples.append(int_2_vocab[ch])\n",
    "      \n",
    "  return ''.join(samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rJZfBauryRxl",
    "outputId": "c0c0d275-5609-4a3b-b81c-dc4172c8eecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints1/i1980_l512.ckpt'"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "colab_type": "code",
    "id": "EuNrXzpSyh4j",
    "outputId": "a6f987c0-8148-425d-f872-a26a93c172fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aunty ji dance of\n",
      "their sendency of the princess with the bith oppeass of his had\n",
      "and heart to her share about the something.\n",
      "\n",
      "\"What was so yut on the discussion of me at a moming, there'voute the\n",
      "distrating one who were not tendenners to say anything in she should\n",
      "not have taken for you to saying while the sectaterest and so\n",
      "an excordent one was at him, and we are a grander so to do with\n",
      "me. That, I don't see her. There will been taken this sunsericalien to\n",
      "arruse to a sparetious at him, and I was a smule of the marming to\n",
      "be a childrong. They had to the bord was strects of mistaken,\n",
      "and he had all all things the charemunass, but so they had been always\n",
      "told them.\n",
      "\n",
      "\n",
      "Consters out of which, husband all has been seeing her. And the\n",
      "praces of the place as she was the solition of his soul in a stort\n",
      "to see all. The carriage and something was in the same.\n",
      "\n",
      "The share one of the propiced said, and then they were set in her\n",
      "exacker. \"In there's a girl when the morning is at it is attacking the beas\n",
      "of his wholenes, and I don't say that he should never say to the does,\n",
      "and they say, and to say so that I have a meeting to say sometime.\"\n",
      "\n",
      "The mother sat so that it was a smarate triad, he show it was so its and\n",
      "straight, and thinking of the condicion, was interrupted her trutting\n",
      "his brother.\n",
      "\n",
      "They should have been sont for the propace.\n",
      "\n",
      "\"What so you can the pacticular and mind of this was in the same imant\n",
      "tried,\" he said.\n",
      "\n",
      "The placation, said to the mad as they said it would be seemed to\n",
      "see a she could be too, and was still and heard.\n",
      "\n",
      "\"To my concinence of the cares of sen on any concellightons to me\n",
      "surposition. And that he's not taken. And you see it to me, he sat\n",
      "so meant that in the saying about the must of his face in the\n",
      "change of the cources. In the corned of the pitter of the same to\n",
      "staid in her mother. And I would be to must a called be such a proffice\n",
      "of the morsen,\" she said, and with answer to the pees to her, and\n",
      "then that in a subject of special continutation in the clearness\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints1')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"aunty ji dance \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA7QLbZv91Fd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "story generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
